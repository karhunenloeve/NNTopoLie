# Persistent Homology of Invertible Autoencoders
Neural networks have been applied in an intangible number of scientific disciplines. Nevertheless, their parameterization is largely unexplored. Dense networks are the coordinate transformations of a manifold from which the data is sampled. After processing the data through a layer, the representation of the original manifold may change. This is crucial for the preservation of its topological structure and should therefore be parameterized correctly. We discuss a method to determine the smallest topology preserving layer for an invertible neural network. We consider each of its layers as an abelian connected matrix group and observe that it is decomposable into tori and real lines. The persistent homology allows us to count the *n*-th homology groups of the input domain. Using KÃ¼nneth's theorem applied to the particular decomposition of the Lie group, we count the *n*-th Betti numbers. Since we know the embedding dimension of some real space and the *1*-sphere we parameterize the bottleneck layer with the smallest possible matrix group that can represent its homology groups. We use residual networks for an efficient alternative to dense networks, due to the dimension of their state space representation. 

**Keywords**: Embedding Dimension, Parametrization, Persistent Homology, Neural Networks and Manifold Learning.
